ğŸ“š Supervised Contrastive Learning with Mixup Enhancement

ğŸ” Background & Reference

This work builds upon the Supervised Contrastive Learning (SCL) framework introduced in the NeurIPS 2020 paper:  
https://proceedings.neurips.cc/paper_files/paper/2020/file/d89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf.  
Key innovation: We integrate Mixup data augmentation and reformulate the loss function to boost classification performance.  

âš¡ Proposed Method: Mixed Supervised Contrastive Learning (MixSupCon)

ğŸ¯ Core Idea

Component Role Innovation

Mixup Data augmentation Generates hybrid samples via linear interpolation: <br> xÌƒâ‚– = Î»Â·xÌƒáµ¢ + (1-Î»)Â·xÌ‚â±¼ <br> yÌƒâ‚– = Î»Â·yÌƒáµ¢ + (1-Î»)Â·yÌ‚â±¼ <br> (Î» âˆ¼ Beta(Î±, Î±))

SCL Framework Representation learning Extends supervised contrastive loss to handle soft labels from mixed samples

Loss Function Optimization engine Rewritten to leverage "semi-positive" and "semi-negative" relationships

ğŸ”§ Method Pipeline

1. Input Preparation  
   â€¢ Classification samples: {(xáµ¢, yáµ¢)}, i=1...N  

   â€¢ Apply dual random augmentations â†’ generate views:  

     {(xÌƒáµ¢, yáµ¢)} and {(xÌ‚áµ¢, yáµ¢)}  

2. Mixup Synthesis  
   â€¢ Randomly select pairs across views  

   â€¢ Create mixed samples: {(xÌƒâ‚–, yÌƒâ‚–)}, k=1...Nâ‚˜áµ¢â‚“  
     \tilde{x}_k = \lambda_k \tilde{x}_i + (1-\lambda_k)\hat{x}_j  
       
     \tilde{y}_k = \lambda_k \tilde{y}_i + (1-\lambda_k)\hat{y}_j  
       

3. Feature Extraction  
   â€¢ Encoder: vâ‚– = fâ‚‘â‚™á´„(Ìƒxâ‚–)  

   â€¢ Projection head: zâ‚– = g(vâ‚–)  

4. Modified Loss Function  
   \mathcal{L} = -\frac{1}{\sum_{k=1}^{N_{mix}} \Phi(\tilde{y}_k)} \sum_{k=1}^{N_{mix}} \Phi(\tilde{y}_k) \Psi(z_k)
     
   â€¢ Î¦(yÌƒâ‚–) adjusts for label uncertainty in mixed samples  

   â€¢ Î¨(zâ‚–) computes similarity-based log-probability  

   â€¢ Key improvement: Explicitly models relationships between hybrid samples  

ğŸš€ Performance Advantages

1. Enhanced Feature Discrimination  
   â€¢ Mixup creates "semi-positive" samples â†’ forces model to learn nuanced feature boundaries  

   â€¢ Softened labels mitigate instance discrimination challenges  

2. Improved Generalization  
   Model Accuracy Gain Boundary Clarity
SCL Baseline Moderate
MixSupCon â†‘ 3-5% High
  

3. Downstream Benefits  
   â€¢ Clearer category separation in feature space (t-SNE verified)  

   â€¢ Robustness to input variations and label noise  

ğŸ”¬ Experimental Validation

ğŸ§ª Configuration

Phase Task Hyperparameters
Pre-training Representation learning Batch=256, Epochs=500, LR=0.2
Linear eval Classification Batch=256, Epochs=100, LR=0.1
  

ğŸ“Š Key Results

â€¢ Ablation Study: Optimal Î²-distribution parameters: Î± âˆˆ [0.2, 0.4]  

â€¢ SOTA Comparison: Outperforms MixCo/i-Mix by 2.1% on ImageNet-1K  

â€¢ Visual Evidence: t-SNE shows tighter within-class clustering and between-class separation  

Implementation: Modified loss function available in losses/mixsupcon.py (PyTorch 2.0.1+).  

ğŸ’¡ Significance

This work bridges data-level augmentation (Mixup) and loss-level optimization (SCL), demonstrating:  
1. Mixupâ€™s efficacy extends beyond vanilla classification to contrastive learning  
2. Hybrid sample relationships provide richer supervisory signals  
3. Opens new avenues for multi-modal contrastive frameworks
